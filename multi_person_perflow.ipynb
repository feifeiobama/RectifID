{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, ToPILImage\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['http_proxy'] = \"\" \n",
    "# os.environ['https_proxy'] = \"\"\n",
    "device_id = 0\n",
    "torch.cuda.set_device('cuda:%d' % device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipeline\n",
    "from piecewise_rectified_flow.src.scheduler_perflow import PeRFlowScheduler\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"hansyan/perflow-sd15-dreamshaper\", safety_checker=None, torch_dtype=torch.float16)\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"hansyan/perflow-sd15-realisticVisionV51\", safety_checker=None, torch_dtype=torch.float16)\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"hansyan/perflow-sd15-disney\", safety_checker=None, torch_dtype=torch.float16)\n",
    "pipe.scheduler = PeRFlowScheduler.from_config(pipe.scheduler.config, prediction_type=\"diff_eps\", num_time_windows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in [pipe.vae, pipe.text_encoder, pipe.unet]:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "pipe.to(\"cuda\")\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "my_forward = pipe.__call__.__wrapped__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import insightface\n",
    "from onnx2torch import convert\n",
    "\n",
    "# antelopev2\n",
    "# https://github.com/deepinsight/insightface/tree/master/python-package#model-zoo\n",
    "detector = insightface.model_zoo.get_model('scrfd_10g_bnkps.onnx', provider_options=[{'device_id': device_id}, {}])\n",
    "detector.prepare(ctx_id=0, input_size=(640, 640))\n",
    "model = convert('glintr100.onnx').eval().to('cuda')\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import kornia\n",
    "\n",
    "\n",
    "ref1 = 'assets/hinton.jpg'\n",
    "# ref1 = 'assets/bengio.jpg'\n",
    "# ref1 = 'assets/schmidhuber.jpg'\n",
    "# ref1 = 'assets/johansson.jpg'\n",
    "# ref1 = 'assets/newton.jpg'\n",
    "# ref2 = 'assets/hinton.jpg'\n",
    "ref2 = 'assets/bengio.jpg'\n",
    "# ref2 = 'assets/schmidhuber.jpg'\n",
    "# ref2 = 'assets/johansson.jpg'\n",
    "# ref2 = 'assets/newton.jpg'\n",
    "\n",
    "ref_image1 = Image.open(ref1).convert(\"RGB\")\n",
    "ref_image2 = Image.open(ref2).convert(\"RGB\")\n",
    "\n",
    "def crop_image_embed(ref_image):\n",
    "    with torch.no_grad():\n",
    "        det_thresh_backup = detector.det_thresh\n",
    "        boxes = []\n",
    "        while len(boxes) == 0:\n",
    "            boxes, kpss = detector.detect(np.array(ref_image), max_num=1)\n",
    "            detector.det_thresh -= 0.1\n",
    "        detector.det_thresh = det_thresh_backup\n",
    "        M = insightface.utils.face_align.estimate_norm(kpss[0])\n",
    "        ref_image_cropped = kornia.geometry.transform.warp_affine(\n",
    "            TF.to_tensor(ref_image).unsqueeze(0).to('cuda'), torch.tensor(M).float().unsqueeze(0).to('cuda'), (112, 112)\n",
    "        ) * 2 - 1\n",
    "\n",
    "        ref_embedding = model(ref_image_cropped)\n",
    "    return ref_image_cropped, ref_embedding\n",
    "\n",
    "ref_image_cropped1, ref_embedding1 = crop_image_embed(ref_image1)\n",
    "ref_image_cropped2, ref_embedding2 = crop_image_embed(ref_image2)\n",
    "\n",
    "cropped_image1 = np.array((ref_image_cropped1[0] / 2 + 0.5).cpu().permute(1, 2, 0) * 255, dtype=np.uint8)\n",
    "cropped_image2 = np.array((ref_image_cropped2[0] / 2 + 0.5).cpu().permute(1, 2, 0) * 255, dtype=np.uint8)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(cropped_image1)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(cropped_image2)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from deepface import DeepFace\n",
    "\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "attribute1 = DeepFace.analyze(img_path=ref1, actions = ['gender', 'race'])\n",
    "attribute2 = DeepFace.analyze(img_path=ref2, actions = ['gender', 'race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = np.argmax([a['region']['w'] * a['region']['h'] for a in attribute1])\n",
    "print(attribute1[idx1]['dominant_gender'], attribute1[idx1]['dominant_race'])\n",
    "\n",
    "idx2 = np.argmax([a['region']['w'] * a['region']['h'] for a in attribute2])\n",
    "print(attribute2[idx2]['dominant_gender'], attribute2[idx2]['dominant_race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "\n",
    "generator = torch.manual_seed(42)\n",
    "\n",
    "latents = nn.Parameter(randn_tensor((4, 4, 64, 64), generator=generator, device=pipe._execution_device, dtype=pipe.text_encoder.dtype))\n",
    "latents0 = latents.data[:1].clone()\n",
    "optimizer = torch.optim.SGD([latents], 1)  # 1 or 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'A person and a person in the snow'\n",
    "# prompt = 'a movie poster of a person and a person'\n",
    "prompt = \"A person and a person having dinner together\"\n",
    "# prompt = \"A person and a person on the beach\"\n",
    "# prompt = \"A person and a person sitting in a park\"\n",
    "# prompt = \"A person and a person holding a bottle of red wine\"\n",
    "# prompt = \"A person and a person standing together\"\n",
    "# prompt = \"A person and a person riding a horse\"\n",
    "\n",
    "if attribute1[idx1]['dominant_gender'] == 'Man':\n",
    "    prompt = prompt.replace('person', attribute1[idx1]['dominant_race'] + ' man', 1)\n",
    "else:\n",
    "    prompt = prompt.replace('person', attribute1[idx1]['dominant_race'] + ' woman', 1)\n",
    "\n",
    "if attribute2[idx2]['dominant_gender'] == 'Man':\n",
    "    prompt = prompt.replace('person', attribute2[idx2]['dominant_race'] + ' man', 1)\n",
    "else:\n",
    "    prompt = prompt.replace('person', attribute2[idx2]['dominant_race'] + ' woman', 1)\n",
    "prompt = prompt + ', closeup'\n",
    "# prompt = prompt + ', faces'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_last = latents.data.clone()\n",
    "latents_last_e = latents.data.clone()\n",
    "initialized_i = -1\n",
    "\n",
    "def callback(self, i, t, callback_kwargs):\n",
    "    global latents_last, latents_last_e, initialized_i\n",
    "    if initialized_i < i:\n",
    "        latents[i:(i+1)].data.copy_(callback_kwargs['latents'])\n",
    "        latents_last[i:(i+1)].copy_(callback_kwargs['latents'])\n",
    "        latents_last_e[i:(i+1)].copy_(callback_kwargs['latents'])\n",
    "        initialized_i = i\n",
    "    if i < 3:\n",
    "        callback_kwargs['latents'] += latents[(i+1):(i+2)] - latents[(i+1):(i+2)].detach()\n",
    "    latents_e = callback_kwargs['latents'].data.clone()\n",
    "    callback_kwargs['latents'] += latents_last[i:(i+1)].detach() - callback_kwargs['latents'].detach()\n",
    "    callback_kwargs['latents'] += latents_e.detach() - latents_last_e[i:(i+1)].detach()\n",
    "    # callback_kwargs['latents'] += latents[i:(i+1)].detach() - latents_last_e[i:(i+1)].detach()\n",
    "    callback_kwargs['latents'] += (latents[i:(i+1)].detach() - latents_last_e[i:(i+1)].detach()) * 0.95796674\n",
    "    latents_last[i:(i+1)].copy_(callback_kwargs['latents'])\n",
    "    latents_last_e[i:(i+1)].data.copy_(latents_e)\n",
    "    latents[i:(i+1)].data.copy_(latents_e)\n",
    "    return callback_kwargs\n",
    "\n",
    "for epoch in tqdm(range(51)):\n",
    "    t0 = time.time()\n",
    "    image = my_forward(pipe, prompt=prompt, num_inference_steps=4, guidance_scale=3.0, latents=latents0+latents[:1]-latents[:1].detach(), output_type='pt', return_dict=False, callback_on_step_end=callback)[0][0]\n",
    "    t1 = time.time()\n",
    "\n",
    "    det_thresh_backup = detector.det_thresh\n",
    "    boxes = []\n",
    "    while len(boxes) <= 1:\n",
    "        boxes, kpss = detector.detect(np.array(image.permute(1, 2, 0).detach().cpu().numpy() * 255, dtype=np.uint8), max_num=2)\n",
    "        detector.det_thresh -= 0.1\n",
    "    det_thresh_backup2 = detector.det_thresh + 0.1\n",
    "    detector.det_thresh = det_thresh_backup\n",
    "    t2 = time.time()\n",
    "\n",
    "    M1 = insightface.utils.face_align.estimate_norm(kpss[0])\n",
    "    image_cropped_1 = kornia.geometry.transform.warp_affine(\n",
    "        image.float().unsqueeze(0), torch.tensor(M1).float().unsqueeze(0).to('cuda'), (112, 112)\n",
    "    ) * 2 - 1\n",
    "    M2 = insightface.utils.face_align.estimate_norm(kpss[1])\n",
    "    image_cropped_2 = kornia.geometry.transform.warp_affine(\n",
    "        image.float().unsqueeze(0), torch.tensor(M2).float().unsqueeze(0).to('cuda'), (112, 112)\n",
    "    ) * 2 - 1\n",
    "    embedding_1 = model(image_cropped_1)\n",
    "    embedding_2 = model(image_cropped_2)\n",
    "    ref_embeddings = torch.cat([ref_embedding1, ref_embedding2, ref_embedding1, ref_embedding2])\n",
    "    proposal_embeddings = torch.cat([embedding_1, embedding_2, embedding_2, embedding_1])\n",
    "    sim = F.cosine_similarity(ref_embeddings, proposal_embeddings)\n",
    "    loss = (2 - max(sim[0] + sim[1], sim[2] + sim[3])) * 100\n",
    "    t3 = time.time()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    t4 = time.time()\n",
    "    grad_norm = latents.grad.reshape(4, -1).norm(dim=-1)\n",
    "    latents.grad /= grad_norm.reshape(4, 1, 1, 1).clamp(min=1)\n",
    "    # latents.grad.clamp_(min=-2e-2, max=2e-2)  # optional for removing artifacts\n",
    "    optimizer.step()\n",
    "    t5 = time.time()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('loss:', loss.data.item())\n",
    "        print('grad:', grad_norm.tolist())\n",
    "        print('time:', t1-t0, t2-t1, '(%f)' % det_thresh_backup2, t3-t2, t4-t3, t5-t4)\n",
    "        plt.imshow(np.array(image.permute(1, 2, 0).detach().cpu() * 255, dtype=np.uint8))\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    del image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

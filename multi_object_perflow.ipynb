{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, ToPILImage\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['http_proxy'] = \"\" \n",
    "# os.environ['https_proxy'] = \"\"\n",
    "device_id = 0\n",
    "torch.cuda.set_device('cuda:%d' % device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipeline\n",
    "from piecewise_rectified_flow.src.scheduler_perflow import PeRFlowScheduler\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"hansyan/perflow-sd15-dreamshaper\", safety_checker=None, torch_dtype=torch.float16)\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"hansyan/perflow-sd15-realisticVisionV51\", safety_checker=None, torch_dtype=torch.float16)\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"hansyan/perflow-sd15-disney\", safety_checker=None, torch_dtype=torch.float16)\n",
    "pipe.scheduler = PeRFlowScheduler.from_config(pipe.scheduler.config, prediction_type=\"diff_eps\", num_time_windows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in [pipe.vae, pipe.text_encoder, pipe.unet]:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "pipe.to(\"cuda\")\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "my_forward = pipe.__call__.__wrapped__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel, OwlViTProcessor, OwlViTForObjectDetection, AutoProcessor\n",
    "from transformers.utils.constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n",
    "\n",
    "detector_processor = OwlViTProcessor.from_pretrained('google/owlvit-base-patch32')\n",
    "detector_processor2 = AutoProcessor.from_pretrained('google/owlvit-base-patch32')\n",
    "detector = OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32').to('cuda')\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-base').to('cuda')\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "OPENAI_CLIP_MEAN = torch.tensor(OPENAI_CLIP_MEAN).to('cuda')\n",
    "OPENAI_CLIP_STD = torch.tensor(OPENAI_CLIP_STD).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import kornia\n",
    "\n",
    "# ref1 = 'can'\n",
    "# ref1 = 'cat'\n",
    "ref1 = 'dog'\n",
    "# ref1 = 'dog3'\n",
    "ref2 = 'can'\n",
    "# ref2 = 'cat'\n",
    "# ref2 = 'dog'\n",
    "# ref2 = 'dog3'\n",
    "\n",
    "extract_name = lambda f: ''.join([i for i in f.replace('_', ' ') if not i.isdigit()])\n",
    "\n",
    "ref_text1 = 'a photo of a %s' % extract_name(ref1)\n",
    "ref_text2 = 'a photo of a %s' % extract_name(ref2)\n",
    "ref_image1 = Image.open('assets/%s_00.jpg' % ref1).convert(\"RGB\")\n",
    "ref_image2 = Image.open('assets/%s_00.jpg' % ref2).convert(\"RGB\")\n",
    "\n",
    "def crop_image_embed(ref_image, ref_text):\n",
    "    with torch.no_grad():\n",
    "        ref_image_detector_processed = detector_processor(text=ref_text, images=ref_image, return_tensors=\"pt\")\n",
    "    ref_image_detector_processed = {a: x.to('cuda') for a, x in ref_image_detector_processed.items()}\n",
    "    detector_outputs = detector(**ref_image_detector_processed)\n",
    "    target_sizes = torch.Tensor([ref_image.size[::-1]])\n",
    "    results = detector_processor.post_process_object_detection(outputs=detector_outputs, target_sizes=target_sizes)\n",
    "    box = results[0]['boxes'][results[0]['scores'].argmax()].tolist()\n",
    "\n",
    "    ref_image_cropped = ref_image.crop(box)\n",
    "    ref_image_processed = processor(images=ref_image_cropped, return_tensors=\"pt\")['pixel_values'].to('cuda')\n",
    "    ref_embedding = model(ref_image_processed)[0][:, 0]\n",
    "    return ref_image_cropped, ref_embedding\n",
    "\n",
    "ref_image_cropped1, ref_embedding1 = crop_image_embed(ref_image1, ref_text1)\n",
    "ref_image_cropped2, ref_embedding2 = crop_image_embed(ref_image2, ref_text2)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(ref_image_cropped1)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(ref_image_cropped2)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "\n",
    "generator = torch.manual_seed(42)\n",
    "\n",
    "latents = nn.Parameter(randn_tensor((4, 4, 64, 64), generator=generator, device=pipe._execution_device, dtype=pipe.text_encoder.dtype))\n",
    "latents0 = latents.data[:1].clone()\n",
    "optimizer = torch.optim.SGD([latents], 2)  # 1 or 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'a %s in the jungle'\n",
    "# prompt = 'a %s in the snow'\n",
    "# prompt = 'a %s on the beach'\n",
    "# prompt = 'a %s on a cobblestone street'\n",
    "# prompt = 'a %s on top of pink fabric'\n",
    "# prompt = 'a %s on top of a wooden floor'\n",
    "# prompt = 'a %s with a city in the background'\n",
    "# prompt = 'a %s with a mountain in the background'\n",
    "# prompt = 'a %s with a blue house in the background'\n",
    "# prompt = 'a %s on top of a purple rug in a forest'\n",
    "# prompt = 'a %s wearing a red hat'\n",
    "# prompt = 'a %s wearing a santa hat'\n",
    "# prompt = 'a %s wearing a rainbow scarf'\n",
    "# prompt = 'a %s wearing a black top hat and a monocle'\n",
    "# prompt = 'a %s in a chef outfit'\n",
    "# prompt = 'a %s in a firefighter outfit'\n",
    "# prompt = 'a %s in a police outfit'\n",
    "# prompt = 'a %s wearing pink glasses'\n",
    "# prompt = 'a %s wearing a yellow shirt'\n",
    "prompt = 'a %s in a purple wizard outfit'\n",
    "# prompt = 'a red %s'\n",
    "# prompt = 'a purple %s'\n",
    "# prompt = 'a shiny %s'\n",
    "# prompt = 'a wet %s'\n",
    "# prompt = 'a cube shaped %s'\n",
    "\n",
    "# prompt = 'a %s with a wheat field in the background'\n",
    "# prompt = 'a %s with a tree and autumn leaves in the background'\n",
    "# prompt = 'a %s with the Eiffel Tower in the background'\n",
    "# prompt = 'a %s floating on top of water'\n",
    "# prompt = 'a %s floating in an ocean of milk'\n",
    "# prompt = 'a %s on top of green grass with sunflowers around it'\n",
    "# prompt = 'a %s on top of a mirror'\n",
    "# prompt = 'a %s on top of the sidewalk in a crowded street'\n",
    "# prompt = 'a %s on top of a dirt road'\n",
    "# prompt = 'a %s on top of a white rug'\n",
    "\n",
    "prompt = prompt % (extract_name(ref1) + ' and a ' + extract_name(ref2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_last = latents.data.clone()\n",
    "latents_last_e = latents.data.clone()\n",
    "initialized_i = -1\n",
    "\n",
    "def callback(self, i, t, callback_kwargs):\n",
    "    global latents_last, latents_last_e, initialized_i\n",
    "    if initialized_i < i:\n",
    "        latents[i:(i+1)].data.copy_(callback_kwargs['latents'])\n",
    "        latents_last[i:(i+1)].copy_(callback_kwargs['latents'])\n",
    "        latents_last_e[i:(i+1)].copy_(callback_kwargs['latents'])\n",
    "        initialized_i = i\n",
    "    if i < 3:\n",
    "        callback_kwargs['latents'] += latents[(i+1):(i+2)] - latents[(i+1):(i+2)].detach()\n",
    "    latents_e = callback_kwargs['latents'].data.clone()\n",
    "    callback_kwargs['latents'] += latents_last[i:(i+1)].detach() - callback_kwargs['latents'].detach()\n",
    "    callback_kwargs['latents'] += latents_e.detach() - latents_last_e[i:(i+1)].detach()\n",
    "    # callback_kwargs['latents'] += latents[i:(i+1)].detach() - latents_last_e[i:(i+1)].detach()\n",
    "    callback_kwargs['latents'] += (latents[i:(i+1)].detach() - latents_last_e[i:(i+1)].detach()) * 0.95796674\n",
    "    latents_last[i:(i+1)].copy_(callback_kwargs['latents'])\n",
    "    latents_last_e[i:(i+1)].data.copy_(latents_e)\n",
    "    latents[i:(i+1)].data.copy_(latents_e)\n",
    "    return callback_kwargs\n",
    "\n",
    "for epoch in tqdm(range(51)):\n",
    "    t0 = time.time()\n",
    "    image = my_forward(pipe, prompt=prompt, num_inference_steps=4, guidance_scale=3.0, latents=latents0+latents[:1]-latents[:1].detach(), output_type='pt', return_dict=False, callback_on_step_end=callback)[0][0]\n",
    "    t1 = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_detector_processed = detector_processor2(images=(image * 255).int(), query_images=ref_image_cropped1, return_tensors=\"pt\")\n",
    "        image_detector_processed = {a: x.to('cuda') for a, x in image_detector_processed.items()}\n",
    "        detector_outputs = detector.image_guided_detection(**image_detector_processed)\n",
    "        target_sizes = torch.Tensor([image.shape[1:]])\n",
    "        results = detector_processor2.post_process_image_guided_detection(outputs=detector_outputs, target_sizes=target_sizes)\n",
    "        box = results[0]['boxes'][results[0]['scores'].argmax()].tolist()\n",
    "        box1 = [min(max(round(b), 0), 512) for b in box]\n",
    "        image_detector_processed = detector_processor2(images=(image * 255).int(), query_images=ref_image_cropped2, return_tensors=\"pt\")\n",
    "        image_detector_processed = {a: x.to('cuda') for a, x in image_detector_processed.items()}\n",
    "        detector_outputs = detector.image_guided_detection(**image_detector_processed)\n",
    "        target_sizes = torch.Tensor([image.shape[1:]])\n",
    "        results = detector_processor2.post_process_image_guided_detection(outputs=detector_outputs, target_sizes=target_sizes)\n",
    "        box = results[0]['boxes'][results[0]['scores'].argmax()].tolist()\n",
    "        box2 = [min(max(round(b), 0), 512) for b in box]\n",
    "    image_cropped1 = TF.crop(image, box1[1], box1[0], box1[3]-box1[1], box1[2]-box1[0])\n",
    "    image_cropped2 = TF.crop(image, box2[1], box2[0], box2[3]-box2[1], box2[2]-box2[0])\n",
    "    t2 = time.time()\n",
    "\n",
    "    image_processed = (F.interpolate(image_cropped1.unsqueeze(0), (224, 224)) - OPENAI_CLIP_MEAN[..., np.newaxis, np.newaxis]) / OPENAI_CLIP_STD[..., np.newaxis, np.newaxis]\n",
    "    embedding = model(image_processed)[0][:, 0]\n",
    "    loss1_1 = (1 - F.cosine_similarity(embedding, ref_embedding1)) * 100\n",
    "    loss1_2 = F.l1_loss(F.interpolate(image_cropped1.unsqueeze(0), (224, 224)), F.interpolate(TF.to_tensor(ref_image_cropped1).half().to('cuda').unsqueeze(0), (224, 224))) * 0  # optional\n",
    "    image_processed = (F.interpolate(image_cropped2.unsqueeze(0), (224, 224)) - OPENAI_CLIP_MEAN[..., np.newaxis, np.newaxis]) / OPENAI_CLIP_STD[..., np.newaxis, np.newaxis]\n",
    "    embedding = model(image_processed)[0][:, 0]\n",
    "    loss2_1 = (1 - F.cosine_similarity(embedding, ref_embedding2)) * 100\n",
    "    loss2_2 = F.l1_loss(F.interpolate(image_cropped2.unsqueeze(0), (224, 224)), F.interpolate(TF.to_tensor(ref_image_cropped2).half().to('cuda').unsqueeze(0), (224, 224))) * 1000  # optional\n",
    "    loss = loss1_1 + loss1_2 + loss2_1 + loss2_2\n",
    "    t3 = time.time()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    t4 = time.time()\n",
    "    grad_norm = latents.grad.reshape(4, -1).norm(dim=-1)\n",
    "    latents.grad /= grad_norm.reshape(4, 1, 1, 1).clamp(min=1)\n",
    "    optimizer.step()\n",
    "    t5 = time.time()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('loss:', loss.data.item())\n",
    "        print('grad:', grad_norm.tolist())\n",
    "        print('time:', t1-t0, t2-t1, t3-t2, t4-t3, t5-t4)\n",
    "        plt.imshow(np.array(image.permute(1, 2, 0).detach().cpu() * 255, dtype=np.uint8))\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    del image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

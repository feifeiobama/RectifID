{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, ToPILImage\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['http_proxy'] = \"\" \n",
    "# os.environ['https_proxy'] = \"\"\n",
    "device_id = 0\n",
    "torch.cuda.set_device('cuda:%d' % device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InstaFlow.code.pipeline_rf import RectifiedFlowPipeline\n",
    "from utils import get_dW_and_merge\n",
    "\n",
    "\n",
    "pipe = RectifiedFlowPipeline.from_pretrained(\"XCLIU/2_rectified_flow_from_sd_1_5\", safety_checker=None, torch_dtype=torch.float16)\n",
    "dW_dict = get_dW_and_merge(pipe, lora_path=\"Lykon/dreamshaper-7\", save_dW=False, alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in [pipe.vae, pipe.text_encoder, pipe.unet]:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "pipe.to(\"cuda\")\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "my_forward = pipe.__call__.__wrapped__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import insightface\n",
    "from onnx2torch import convert\n",
    "\n",
    "# antelopev2\n",
    "# https://github.com/deepinsight/insightface/tree/master/python-package#model-zoo\n",
    "detector = insightface.model_zoo.get_model('scrfd_10g_bnkps.onnx', provider_options=[{'device_id': device_id}, {}])\n",
    "detector.prepare(ctx_id=0, input_size=(640, 640))\n",
    "model = convert('glintr100.onnx').eval().to('cuda')\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import kornia\n",
    "\n",
    "ref = 'assets/hinton.jpg'\n",
    "# ref = 'assets/bengio.jpg'\n",
    "# ref = 'assets/schmidhuber.jpg'\n",
    "# ref = 'assets/johansson.jpg'\n",
    "# ref = 'assets/newton.jpg'\n",
    "\n",
    "ref_image = Image.open(ref).convert(\"RGB\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    det_thresh_backup = detector.det_thresh\n",
    "    boxes = []\n",
    "    while len(boxes) == 0:\n",
    "        boxes, kpss = detector.detect(np.array(ref_image), max_num=1)\n",
    "        detector.det_thresh -= 0.1\n",
    "    detector.det_thresh = det_thresh_backup\n",
    "    M = insightface.utils.face_align.estimate_norm(kpss[0])\n",
    "    ref_image_cropped = kornia.geometry.transform.warp_affine(\n",
    "        TF.to_tensor(ref_image).unsqueeze(0).to('cuda'), torch.tensor(M).float().unsqueeze(0).to('cuda'), (112, 112)\n",
    "    ) * 2 - 1\n",
    "\n",
    "    ref_embedding = model(ref_image_cropped)\n",
    "\n",
    "cropped_image = np.array((ref_image_cropped[0] / 2 + 0.5).cpu().permute(1, 2, 0) * 255, dtype=np.uint8)\n",
    "plt.imshow(cropped_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from deepface import DeepFace\n",
    "\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "attribute = DeepFace.analyze(img_path=ref, actions = ['gender', 'race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmax([a['region']['w'] * a['region']['h'] for a in attribute])\n",
    "print(attribute[idx]['dominant_gender'], attribute[idx]['dominant_race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "\n",
    "generator = torch.manual_seed(42)\n",
    "\n",
    "latents = nn.Parameter(randn_tensor((4, 4, 64, 64), generator=generator, device=pipe._execution_device, dtype=pipe.text_encoder.dtype))\n",
    "latents0 = latents.data[:1].clone()\n",
    "optimizer = torch.optim.SGD([latents], 1)  # 1 or 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Selfie of a middle-aged person on a yacht'\n",
    "# prompt = 'A photo of a person wearing a black suit, holding red roses in hand, upper body, behind is the Eiffel Tower'\n",
    "# prompt = 'a man sitting in the cafe, comic, graphic illustration, comic art, graphic novel art, vibrant, highly detailed, colored, 2d minimalistic'\n",
    "\n",
    "# prompt = 'a photo of a person'\n",
    "# prompt = 'a person with a sad expression'\n",
    "# prompt = 'a person with a happy expression'\n",
    "# prompt = 'a person with a puzzled expression'\n",
    "# prompt = 'a person with an angry expression'\n",
    "# prompt = 'a person plays the LEGO toys'\n",
    "# prompt = 'a person on the beach'\n",
    "# prompt = 'a person piloting a fighter jet'\n",
    "# prompt = 'a person wearing the sweater, a backpack and camping stove, outdoors, RAW, ultra high res'\n",
    "# prompt = 'a person wearing a scifi spacesuit in space'\n",
    "# prompt = 'cubism painting of a person'\n",
    "# prompt = 'fauvism painting of a person'\n",
    "# prompt = 'cave mural depicting a person'\n",
    "# prompt = 'pointillism painting of a person'\n",
    "# prompt = 'a person latte art'\n",
    "\n",
    "if attribute[idx]['dominant_gender'] == 'Man':\n",
    "    prompt = prompt.replace('person', attribute[idx]['dominant_race'] + ' man')\n",
    "else:\n",
    "    prompt = prompt.replace('person', attribute[idx]['dominant_race'] + ' woman')\n",
    "# prompt = prompt + ', face'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_last = latents.data.clone()\n",
    "latents_last_e = latents.data.clone()\n",
    "initialized_i = -1\n",
    "\n",
    "def callback(self, i, t, callback_kwargs):\n",
    "    global latents_last, latents_last_e, initialized_i\n",
    "    if initialized_i < i:\n",
    "        latents[i:(i+1)].data.copy_(callback_kwargs['latents'])\n",
    "        latents_last[i:(i+1)].copy_(callback_kwargs['latents'])\n",
    "        latents_last_e[i:(i+1)].copy_(callback_kwargs['latents'])\n",
    "        initialized_i = i\n",
    "    if i < 3:\n",
    "        callback_kwargs['latents'] += latents[(i+1):(i+2)] - latents[(i+1):(i+2)].detach()\n",
    "    latents_e = callback_kwargs['latents'].data.clone()\n",
    "    callback_kwargs['latents'] += latents[i:(i+1)].detach() - callback_kwargs['latents'].detach()\n",
    "    callback_kwargs['latents'] += latents_last[i:(i+1)].detach() - latents_last_e[i:(i+1)].detach()\n",
    "    latents_last[i:(i+1)].copy_(callback_kwargs['latents'])\n",
    "    latents_last_e[i:(i+1)].data.copy_(latents_e)\n",
    "    latents[i:(i+1)].data.copy_(latents_e)\n",
    "    return callback_kwargs\n",
    "\n",
    "for epoch in tqdm(range(51)):\n",
    "    t0 = time.time()\n",
    "    image = my_forward(pipe, prompt=prompt, num_inference_steps=4, guidance_scale=1.5, latents=latents0+latents[:1]-latents[:1].detach(), output_type='pt', return_dict=False, callback_on_step_end=callback)[0][0]\n",
    "    t1 = time.time()\n",
    "\n",
    "    det_thresh_backup = detector.det_thresh\n",
    "    boxes = []\n",
    "    while len(boxes) == 0:\n",
    "        boxes, kpss = detector.detect(np.array(image.permute(1, 2, 0).detach().cpu().numpy() * 255, dtype=np.uint8), max_num=1)\n",
    "        detector.det_thresh /= 2\n",
    "    det_thresh_backup2 = detector.det_thresh * 2\n",
    "    detector.det_thresh = det_thresh_backup\n",
    "    t2 = time.time()\n",
    "    \n",
    "    M = insightface.utils.face_align.estimate_norm(kpss[0])\n",
    "    image_cropped = kornia.geometry.transform.warp_affine(\n",
    "        image.float().unsqueeze(0), torch.tensor(M).float().unsqueeze(0).to('cuda'), (112, 112)\n",
    "    ) * 2 - 1\n",
    "    embedding = model(image_cropped)\n",
    "    loss = (1 - F.cosine_similarity(embedding, ref_embedding)) * 100\n",
    "    t3 = time.time()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    t4 = time.time()\n",
    "    grad_norm = latents.grad.reshape(4, -1).norm(dim=-1)\n",
    "    latents.grad /= grad_norm.reshape(4, 1, 1, 1).clamp(min=1)\n",
    "    optimizer.step()\n",
    "    t5 = time.time()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('loss:', loss.data)\n",
    "        print('grad:', grad_norm)\n",
    "        print('time:', t1-t0, t2-t1, '(%f)' % det_thresh_backup2, t3-t2, t4-t3, t5-t4)\n",
    "        plt.imshow(np.array(image.permute(1, 2, 0).detach().cpu() * 255, dtype=np.uint8))\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    del image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
